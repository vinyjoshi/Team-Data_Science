# Team-Data_Science
<h1 align="center">Hi ðŸ‘‹, we are "Team-Data_Science"</h1>
<h3 align="center">A team of 20 members for HackBio'2021 Virtual Bioinformatics Internship</h3>
- ðŸ”­ We are currently working on <a href="https://github.com/Bhushan-Wagh025/Team-Data_Science">**Team-Data_Science**</a>

- ðŸ‘¯ HackBio Channel [https://hackbio-internship.github.io/webpage-test/](https://hackbio-internship.github.io/webpage-test/)
- ðŸ“« How to reach us **waghbhushan1123@gmail.com**

<h3 align="left">Language:</h3>
<a href="https://www.python.org" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" alt="python" width="40" height="40"/> </a>

### Team Work
**To build a model to accurately detect the presence of Parkinsonâ€™s Disease in an individual.**

## Algorithm used initially: 
<h4 align="center">XGBoost</h3>
XGBoost is a new Machine Learning algorithm designed with speed and performance in mind. XGBoost stands for eXtreme Gradient Boosting and is based on decision trees. In this project, we will import the XGBClassifier from the xgboost library; this is an implementation of the scikit-learn API for XGBoost classification.



## Python libraries used:
scikit-learn, numpy, pandas, matplotlib and xgboost

## Python Built in Packages used:
json warnings re

In this Python machine learning project, using the <a href="https://data-flair.training/blogs/python-libraries/">Python libraries</a> scikit-learn, numpy, pandas, and xgboost, we will build a model using an XGBClassifier. Weâ€™ll load the data, get the features and labels, scale the features, then split the dataset, build an XGBClassifier, and then calculate the accuracy of our model.

Dataset used:
Youâ€™ll need the UCI ML Parkinsons dataset for this; you can <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/">download it here.</a> The dataset has 24 columns and 195 records and is only 39.7 KB.


## Other Algorithms which you have to use to analyse the data after we have used XGBClassifier, and draw comparisons between the results (choose any one) :

## Beginners:
<h5 align="center"><br>logistic regression<br></h5>

## Intermediate/Advanced:
<h6 align="center">k-nearest neighbors<br>
support vector machines<br>
random forests<br>
bagging algorithms<br>
Naive Bayes</h6>

## Apart from this we will be generating graphical representation for our data and results:
<h7 align="center">Histograms<br>
Scatter Plot<br>
Heatmaps<br>
Bar Charts<br>
Box<br>
Etc</h7>














<p align="center">
<p3><a href=""><img align="center" src="https://camo.githubusercontent.com/c58e07fb34a45fd051183258b5860608dd86ac98dd151d0522e0575966082b88/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f6e706d2f73696d706c652d69636f6e7340332e302e312f69636f6e732f747769747465722e737667" alt="tbi_internship" height="20" width="20" data-canonical-src="https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/twitter.svg" style="max-width:100%;"></p3></a>
<p3><a href=""><img align="center" src="https://camo.githubusercontent.com/aecaf87326884e8b0466bb799265a13fee7586246ebda3e066cb7fad82a1fd23/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f6e706d2f73696d706c652d69636f6e7340332e302e312f69636f6e732f696e7374616772616d2e737667" alt="ssiddhaantsharma" height="20" width="20" data-canonical-src="https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/instagram.svg" style="max-width:100%;"></a></p3>
<p3><a href =""><img align="center" src="https://camo.githubusercontent.com/4a20e861b6593d07cef8e8b740e64a866ba7a9916d7e00a9c50c05e93a8096b8/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f6e706d2f73696d706c652d69636f6e7340332e302e312f69636f6e732f796f75747562652e737667" alt="ucrp4skeqrnbax0od3ybyt1w" height="20" width="20" data-canonical-src="https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/youtube.svg" style="max-width:100%;"></a></p3></p>
